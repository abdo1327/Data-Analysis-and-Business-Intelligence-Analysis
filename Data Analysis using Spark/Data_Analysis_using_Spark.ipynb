{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SyNbXiFe_qy"
      },
      "source": [
        "# Data Analysis using Spark\n",
        "\n",
        " create a DataFrame by loading data from a CSV file and apply transformations and actions using Spark SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFI82CaNe_q0",
        "outputId": "7e39fe62-9929-4701-e821-6b4fd6fec8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark, wget\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=879804f9a02535f2f333cec59f8e64bc49709b264f2ffccb4834cc1040989bcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=e9837c81733377af37de71a1a6398a24feb9fdcab1a565a6ea2ff4bb7e7fed27\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built pyspark wget\n",
            "Installing collected packages: wget, findspark, pyspark\n",
            "Successfully installed findspark-2.0.1 pyspark-3.5.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "\n",
        "!pip install pyspark  findspark wget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "1T15b_PNe_q-"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "o46T-BOqe_rA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "4vBsTqRle_rB"
      },
      "outputs": [],
      "source": [
        "# Creating a SparkContext object\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Creating a SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rWLJhcxe_rC"
      },
      "source": [
        "2. Download the CSV data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gm-flBRge_rC",
        "outputId": "494b90c1-4ce5-4bb0-efaa-390d383cc646"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'employees.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import wget\n",
        "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zKgbWsFe_rD"
      },
      "source": [
        "####  Generate a Spark DataFrame from the CSV data\n",
        "\n",
        "Read data from the provided CSV file, `employees.csv` and import it into a Spark DataFrame variable named `employees_df`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "01XUXt0-e_rD"
      },
      "outputs": [],
      "source": [
        "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcqwCDDye_rE"
      },
      "source": [
        "####  Define a schema for the data\n",
        "\n",
        "Construct a schema for the input data and then utilize the defined schema to read the CSV file to create a DataFrame named `employees_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "BNBv0Lkse_rE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Emp_No\", IntegerType(), True),\n",
        "    StructField(\"Emp_Name\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Department\", StringType(), True)\n",
        "])\n",
        "employees_df = spark.read.csv(\"employees.csv\", header=True, schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi-q2We4e_rE"
      },
      "source": [
        "####  Display schema of DataFrame\n",
        "\n",
        "Display the schema of the `employees_df` DataFrame, showing all columns and their respective data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgY54T2He_rE",
        "outputId": "3f0237a1-297e-406f-974c-451e9b4e4b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Emp_No: integer (nullable = true)\n",
            " |-- Emp_Name: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display all columns of the DataFrame, along with their respective data types\n",
        "employees_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_AGKzL3e_rE"
      },
      "source": [
        "####  Create a temporary view\n",
        "\n",
        "Create a temporary view named `employees` for the `employees_df` DataFrame, enabling Spark SQL queries on the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "id": "UkH0GZROe_rF"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view named \"employees\" for the DataFrame\n",
        "employees_df.createOrReplaceTempView(\"employees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0PVSnuEe_rF"
      },
      "source": [
        "####  Execute an SQL query\n",
        "\n",
        "Compose and execute an SQL query to fetch the records from the `employees` view where the age of employees exceeds 30. Then, display the result of the SQL query, showcasing the filtered records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD4rSSdde_rF",
        "outputId": "e4d3190c-a95a-449b-a84c-0ac622b0d65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+---+----------+\n",
            "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
            "+------+-----------+------+---+----------+\n",
            "|   199|    Douglas|  2600| 34|     Sales|\n",
            "|   200|   Jennifer|  4400| 36| Marketing|\n",
            "|   201|    Michael| 13000| 32|        IT|\n",
            "|   202|        Pat|  6000| 39|        HR|\n",
            "|   203|      Susan|  6500| 36| Marketing|\n",
            "|   205|    Shelley| 12008| 33|   Finance|\n",
            "|   206|    William|  8300| 37|        IT|\n",
            "|   100|     Steven| 24000| 39|        IT|\n",
            "|   102|        Lex| 17000| 37| Marketing|\n",
            "|   103|  Alexander|  9000| 39| Marketing|\n",
            "|   104|      Bruce|  6000| 38|        IT|\n",
            "|   105|      David|  4800| 39|        IT|\n",
            "|   106|      Valli|  4800| 38|     Sales|\n",
            "|   107|      Diana|  4200| 35|     Sales|\n",
            "|   109|     Daniel|  9000| 35|        HR|\n",
            "|   110|       John|  8200| 31| Marketing|\n",
            "|   111|     Ismael|  7700| 32|        IT|\n",
            "|   112|Jose Manuel|  7800| 34|        HR|\n",
            "|   113|       Luis|  6900| 34|     Sales|\n",
            "|   116|     Shelli|  2900| 37|   Finance|\n",
            "+------+-----------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM employees WHERE age>30\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFQyJF56e_rF"
      },
      "source": [
        "#### Calculate Average Salary by Department\n",
        "\n",
        "Compose an SQL query to retrieve the average salary of employees grouped by department. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi_rSImve_rF",
        "outputId": "8776ed28-4819-4903-92e2-1e8b7cde9057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|Department|             avgr|\n",
            "+----------+-----------------+\n",
            "|     Sales|5492.923076923077|\n",
            "|        HR|           5837.5|\n",
            "|   Finance|           5730.8|\n",
            "| Marketing|6633.333333333333|\n",
            "|        IT|           7400.0|\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL query to calculate the average salary of employees grouped by department\n",
        "spark.sql(\"SELECT Department, AVG(Salary) AS avgr FROM employees GROUP BY Department\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR2fET7ee_rF"
      },
      "source": [
        "####  Filter and Display IT Department Employees\n",
        "\n",
        "Apply a filter on the `employees_df` DataFrame to select records where the department is `'IT'`. Display the filtered DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqqMYYGXe_rG",
        "outputId": "353698b6-91e4-4d97-f95a-444525ebded2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------+---+----------+\n",
            "|Emp_No|Emp_Name|Salary|Age|Department|\n",
            "+------+--------+------+---+----------+\n",
            "|   198|  Donald|  2600| 29|        IT|\n",
            "|   201| Michael| 13000| 32|        IT|\n",
            "|   206| William|  8300| 37|        IT|\n",
            "|   100|  Steven| 24000| 39|        IT|\n",
            "|   104|   Bruce|  6000| 38|        IT|\n",
            "|   105|   David|  4800| 39|        IT|\n",
            "|   111|  Ismael|  7700| 32|        IT|\n",
            "|   129|   Laura|  3300| 38|        IT|\n",
            "|   132|      TJ|  2100| 34|        IT|\n",
            "|   136|   Hazel|  2200| 29|        IT|\n",
            "+------+--------+------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply a filter to select records where the department is 'IT'\n",
        "spark.sql(\"SELECT *  FROM employees WHERE Department='IT'\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xriSOiJbe_rG"
      },
      "source": [
        "####  Add 10% Bonus to Salaries\n",
        "\n",
        "Perform a transformation to add a new column named \"SalaryAfterBonus\" to the DataFrame. Calculate the new salary by adding a 10% bonus to each employee's salary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWBpicgHe_rG",
        "outputId": "25deaaba-c6b4-4482-87ae-c926ea707b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyArrow in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from PyArrow) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyArrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [],
        "id": "-M-dM53ke_rG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
        "employees_df = employees_df.withColumn(\"SalaryAfterBonus\", expr(\"Salary * 1.1\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Ng8Nrfe_rH"
      },
      "source": [
        "####  Maximum Salary by Age\n",
        "\n",
        "Group the data by age and calculate the maximum salary for each age group. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esQ8i7cne_rH",
        "outputId": "54496bd5-8cf5-402f-b872-fd6bca63b5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+\n",
            "|max(Salary)|maxsal|\n",
            "+-----------+------+\n",
            "|       8200|    31|\n",
            "|       7800|    34|\n",
            "|      12008|    28|\n",
            "|      17000|    27|\n",
            "|       3600|    26|\n",
            "|      17000|    37|\n",
            "|       9000|    35|\n",
            "|      24000|    39|\n",
            "|       6000|    38|\n",
            "|      10000|    29|\n",
            "|      13000|    32|\n",
            "|      12008|    33|\n",
            "|       8000|    30|\n",
            "|       7900|    36|\n",
            "+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Group data by age and calculate the maximum salary for each age group\n",
        "spark.sql(\"SELECT max(Salary),Age as maxsal  FROM employees GROUP BY Age\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKkK5Wy6e_rH"
      },
      "source": [
        "####  Self-Join on Employee Data\n",
        "\n",
        "Join the \"employees_df\" DataFrame with itself based on the \"Emp_No\" column. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p126zkI4e_rH",
        "outputId": "3722629a-0d9e-4b46-cc7a-fb6a9d76781f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[Emp_No: int, Emp_Name: string, Salary: int, Age: int, Department: string, SalaryAfterBonus: decimal(13,1), Emp_Name: string, Salary: int, Age: int, Department: string, SalaryAfterBonus: decimal(13,1)]\n"
          ]
        }
      ],
      "source": [
        "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
        "joined_df = employees_df.join(employees_df, 'Emp_No', 'inner')\n",
        "print(joined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRodFT2He_rH"
      },
      "source": [
        "#### Calculate Average Employee Age\n",
        "\n",
        "Calculate the average age of employees using the built-in aggregation function. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIb98MZee_rH",
        "outputId": "1f2fa249-74bc-431d-8b80-9994fb977cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|AverageAge|\n",
            "+----------+\n",
            "|     33.56|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average age of employees\n",
        "from pyspark.sql.functions import avg\n",
        "average_age_df = employees_df.agg(avg(\"Age\").alias(\"AverageAge\"))\n",
        "average_age_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epWIHx9Le_rK"
      },
      "source": [
        "#### Calculate Total Salary by Department\n",
        "\n",
        "Calculate the total salary for each department using the built-in aggregation function. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNC735Mxe_rK",
        "outputId": "e0b5f288-c5f7-4a00-f2a5-6ed411a4fb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|Department|SumOfSal|\n",
            "+----------+--------+\n",
            "|     Sales|   71408|\n",
            "|        HR|   46700|\n",
            "|   Finance|   57308|\n",
            "| Marketing|   59700|\n",
            "|        IT|   74000|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the total salary for each department. Hint - User GroupBy and Aggregate functions\n",
        "from pyspark.sql.functions import sum\n",
        "sum_dep_df = employees_df.groupBy('Department').agg(sum('Salary').alias('SumOfSal'))\n",
        "sum_dep_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T113usk-e_rK"
      },
      "source": [
        "#### Sort Data by Age and Salary\n",
        "\n",
        "Apply a transformation to sort the DataFrame by age in ascending order and then by salary in descending order. Display the sorted DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lPT5xn7je_rK"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
        "sorted_df = employees_df.orderBy(col(\"Age\").asc(), col(\"Salary\").desc())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "king1480e_rL"
      },
      "source": [
        "#### Count Employees in Each Department\n",
        "\n",
        "Calculate the number of employees in each department. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQcsqMMee_rM",
        "outputId": "4262924d-857f-4b54-9f0a-b4cdee9bfe01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|countofemb|\n",
            "+----------+----------+\n",
            "|     Sales|        13|\n",
            "|        HR|         8|\n",
            "|   Finance|        10|\n",
            "| Marketing|         9|\n",
            "|        IT|        10|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Calculate the number of employees in each department\n",
        "count_dep_df = employees_df.groupBy('Department').agg(count('Emp_No').alias('countofemb'))\n",
        "count_dep_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGp78h1e_rN"
      },
      "source": [
        "####  Filter Employees with the letter o in the Name\n",
        "\n",
        "Apply a filter to select records where the employee's name contains the letter `'o'`. Display the filtered DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjgHjMlve_rN",
        "outputId": "5ae8af4e-d5f0-4c33-a58d-9e193f136669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+---+----------+\n",
            "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
            "+------+-----------+------+---+----------+\n",
            "|   198|     Donald|  2600| 29|        IT|\n",
            "|   199|    Douglas|  2600| 34|     Sales|\n",
            "|   110|       John|  8200| 31| Marketing|\n",
            "|   112|Jose Manuel|  7800| 34|        HR|\n",
            "|   130|      Mozhe|  2800| 28| Marketing|\n",
            "|   133|      Jason|  3300| 38|     Sales|\n",
            "|   139|       John|  2700| 36|     Sales|\n",
            "|   140|     Joshua|  2500| 29|   Finance|\n",
            "+------+-----------+------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
        "spark.sql(\"SELECT * FROM employees WHERE Emp_Name LIKE '%o%'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBVzSMVLe_rN"
      },
      "source": [
        "####  This project was created as part of earning the IBM Data Engineering Professional Certificate, using the work from Raghul Ramesh & Lavanya T S"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}